{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQt-gyAYUbm3"
      },
      "source": [
        "### AI/LLM Engineering Kick-off!! \n",
        "\n",
        "\n",
        "For our initial activity, we will be using the OpenAI Library to Programmatically Access GPT-4.1-nano!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PInACkIWUhOd"
      },
      "source": [
        "In order to get started, you'll need an OpenAI API Key. [here](https://platform.openai.com)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecnJouXnUgKv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Initialize the OpenAI client\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1pOrbwSU5H_"
      },
      "source": [
        "### Our First Prompt\n",
        "\n",
        "You can reference OpenAI's [documentation](https://platform.openai.com/docs/api-reference/chat) if you get stuck!\n",
        "\n",
        "Let's create a `ChatCompletion` model to kick things off!\n",
        "\n",
        "There are three \"roles\" available to use:\n",
        "\n",
        "- `developer`\n",
        "- `assistant`\n",
        "- `user`\n",
        "\n",
        "OpenAI provides some context for these roles [here](https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages)\n",
        "\n",
        "Let's just stick to the `user` role for now and send our first message to the endpoint!\n",
        "\n",
        "If we check the documentation, we'll see that it expects it in a list of prompt objects - so we'll be sure to do that!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iy_LEPNEMVvC"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofMwuUQOU4sf",
        "outputId": "7db141d5-7f7a-4f82-c9ff-6eeafe65cfa6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangChain and LlamaIndex (formerly known as GPT Index) are both popular frameworks designed to facilitate the development of AI-powered applications involving language models and data integration. While they share similarities in enabling developers to build more sophisticated AI solutions, they focus on different aspects and offer distinct functionalities. Here's a comparison highlighting their key differences:\n",
            "\n",
            "1. Purpose and Focus\n",
            "   - LangChain:\n",
            "     - Focus: Building end-to-end applications that leverage language models, especially for tasks like conversational agents, chatbots, and complex multi-step workflows.\n",
            "     - Emphasis: Chains, prompts, memory management, agent frameworks, and integrating various tools and APIs.\n",
            "   - LlamaIndex:\n",
            "     - Focus: Simplified ingestion, indexing, and querying of external data sources (documents, PDFs, databases) to enable efficient retrieval-augmented generation (RAG) with language models.\n",
            "     - Emphasis: Data management, indexing, retrieval, and making external data accessible for language models.\n",
            "\n",
            "2. Core Functionality\n",
            "   - LangChain:\n",
            "     - Provides abstractions such as chains, prompts, and agents to orchestrate complex sequences of interactions with language models.\n",
            "     - Supports integration with multiple APIs, tools, and knowledge sources.\n",
            "     - Offers memory management to maintain context across interactions.\n",
            "   - LlamaIndex:\n",
            "     - Enables loading various data sources and creating optimized indexes.\n",
            "     - Facilitates fast retrieval of relevant data snippets during query processing.\n",
            "     - Designed to enhance language models' ability to access and reason over specific document collections.\n",
            "\n",
            "3. Use Cases\n",
            "   - LangChain:\n",
            "     - Building chatbots, virtual assistants, or AI agents that require complex reasoning, multi-turn conversations, or tool integrations.\n",
            "     - Orchestrating workflows that combine language models with APIs and custom logic.\n",
            "   - LlamaIndex:\n",
            "     - Developing applications that need to answer questions based on large document collections.\n",
            "     - Enhancing language models with access to external knowledge bases for retrieval-augmented generation.\n",
            "\n",
            "4. Integration and Extensibility\n",
            "   - LangChain:\n",
            "     - Highly modular with support for numerous language models, APIs, and custom components.\n",
            "     - Extensive community and ecosystem for chaining together different modules.\n",
            "   - LlamaIndex:\n",
            "     - Provides multiple index types (e.g., tree, list, GPT-based indexing) to suit different data needs.\n",
            "     - Easier setup for the purpose of data retrieval workflows.\n",
            "\n",
            "**Summary:**\n",
            "- **LangChain** is a comprehensive framework aimed at building complex language model applications, emphasizing orchestration, conversation management, and multi-step workflows.\n",
            "- **LlamaIndex** specializes in indexing and retrieving data from external sources to augment language model responses, primarily focusing on efficient data access and retrieval.\n",
            "\n",
            "Depending on your project requirements—whether you're building an interactive application with complex logic (LangChain) or integrating and querying large datasets (LlamaIndex)—you might choose one or even use both together for a hybrid solution.\n",
            "\n",
            "---\n",
            "\n",
            "If you'd like more details on specific features or use cases, feel free to ask!\n"
          ]
        }
      ],
      "source": [
        "YOUR_PROMPT = \"What is the difference between LangChain and LlamaIndex?\"\n",
        "\n",
        "resp = client.chat.completions.create(\n",
        "    model=\"gpt-4.1-nano\",\n",
        "    messages=[{\"role\": \"user\", \"content\": YOUR_PROMPT}],\n",
        ")\n",
        "\n",
        "print(resp.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX-7MnFhVNoT"
      },
      "source": [
        "As you can see, the prompt comes back with a tonne of information that we can use when we're building our applications!\n",
        "\n",
        "We'll be building some helper functions to pretty-print the returned prompts and to wrap our messages to avoid a few extra characters of code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB76LJrDVgbc"
      },
      "source": [
        "##### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-vmtUV7WVOLW"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def get_response(client: OpenAI, messages: str, model: str = \"gpt-4.1-nano\") -> str:\n",
        "    return client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "def system_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"developer\", \"content\": message}\n",
        "\n",
        "def assistant_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"assistant\", \"content\": message}\n",
        "\n",
        "def user_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"user\", \"content\": message}\n",
        "\n",
        "def pretty_print(message: str) -> str:\n",
        "    display(Markdown(message.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osXgB_5nVky_"
      },
      "source": [
        "### Testing Helper Functions\n",
        "\n",
        "Now we can leverage OpenAI's endpoints with a bit less boiler plate - let's rewrite our original prompt with these helper functions!\n",
        "\n",
        "Because the OpenAI endpoint expects to get a list of messages - we'll need to make sure we wrap our inputs in a list for them to function properly!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "4yRwAWvgWFNq",
        "outputId": "777e7dcb-43e3-491a-d94a-f543e19b61e6"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "LangChain and LlamaIndex (formerly known as GPT Index) are both popular frameworks designed to facilitate the development of AI applications involving large language models (LLMs) and external data sources. However, they serve different primary purposes and have distinct features. Here's a comparison to clarify their differences:\n",
              "\n",
              "**1. Purpose and Focus**\n",
              "\n",
              "- **LangChain:**\n",
              "  - Focuses on building **Conversational AI applications**, including chatbots, agents, and complex multi-step workflows.\n",
              "  - Provides abstractions for prompt management, memory, chains (sequences of calls), and integration with various LLM providers.\n",
              "  - Emphasizes modularity and composability for constructing complex language model applications.\n",
              "\n",
              "- **LlamaIndex (GPT Index):**\n",
              "  - Designed to enable **efficient indexing and querying** of large external data sources (documents, knowledge bases) using LLMs.\n",
              "  - Facilitates creating **embeddings, indexes, and retrieval mechanisms** so LLMs can access structured or unstructured data effectively.\n",
              "  - Focuses on **knowledge retrieval and question-answering** over large datasets.\n",
              "\n",
              "**2. Core Functionality**\n",
              "\n",
              "- **LangChain:**\n",
              "  - Chain management: sequential or branching call chains.\n",
              "  - Memory handling for maintaining context across interactions.\n",
              "  - Agents that can decide which tools to invoke based on input.\n",
              "  - Support for prompt templating and LLM customization.\n",
              "  - Integrates with a variety of data sources and APIs.\n",
              "\n",
              "- **LlamaIndex:**\n",
              "  - Data ingestion: parsing and processing large text corpora.\n",
              "  - Building various types of indexes (e.g., simple, tree, graph-based).\n",
              "  - Embedding and retrieval mechanisms tailored for data QA.\n",
              "  - Ease of querying large datasets with LLMs by incorporating retrieved context into prompts.\n",
              "\n",
              "**3. Use Cases**\n",
              "\n",
              "- **LangChain:**\n",
              "  - Building chatbots and conversational agents.\n",
              "  - Orchestrating multi-step workflows involving multiple APIs or tools.\n",
              "  - Managing dialogue history and state.\n",
              "  - Developing autonomous agents that can perform complex tasks.\n",
              "\n",
              "- **LlamaIndex:**\n",
              "  - Creating searchable indexes over large documentation, PDFs, or static datasets.\n",
              "  - Building question-answering systems that access external data.\n",
              "  - Data integration for LLMs where data is too large to embed directly in prompts.\n",
              "\n",
              "**4. Ecosystem and Community**\n",
              "\n",
              "- **LangChain:**\n",
              "  - Broader adoption among developers building conversational AI.\n",
              "  - Rich documentation and examples for various applications.\n",
              "  - Supports multiple LLM providers and integrations.\n",
              "\n",
              "- **LlamaIndex:**\n",
              "  - Focused on data ingestion, indexing, and retrieval.\n",
              "  - Growing community around document retrieval and knowledge bases.\n",
              "  - Works well with large datasets and retrieval-augmented generation (RAG) workflows.\n",
              "\n",
              "---\n",
              "\n",
              "**In summary:**\n",
              "\n",
              "| Aspect                   | LangChain                                        | LlamaIndex (GPT Index)                                 |\n",
              "|---------------------------|--------------------------------------------------|---------------------------------------------------------|\n",
              "| Primary Focus             | Building conversational AI and workflows        | Indexing and querying large external data sources    |\n",
              "| Core Functionality        | Chain management, prompt handling, agents        | Data ingestion, indexing, retrieval                    |\n",
              "| Typical Use Cases         | Chatbots, multi-step workflows, agents           | Document QA, knowledge base retrieval, RAG systems   |\n",
              "| Integration & Ecosystem   | Extensive integrations, multi-LLM support      | Focused on data indexing and retrieval mechanisms     |\n",
              "\n",
              "**Choosing between them** depends on your project needs:\n",
              "- Use **LangChain** if you're building interactive, multi-step language applications and need flexible orchestration.\n",
              "- Use **LlamaIndex** if you need to process, index, and retrieve information from large datasets to support your LLM-based solutions.\n",
              "\n",
              "---\n",
              "\n",
              "Let me know if you'd like more detailed comparisons or examples!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "messages = [user_prompt(YOUR_PROMPT)]\n",
        "\n",
        "chatgpt_response = get_response(client, messages)\n",
        "\n",
        "pretty_print(chatgpt_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPs3ScS1WpoC"
      },
      "source": [
        "Let's focus on extending this a bit, and incorporate a `developer` message as well!\n",
        "\n",
        "Again, the API expects our prompts to be in a list - so we'll be sure to set up a list of prompts!\n",
        "\n",
        ">REMINDER: The `developer` message acts like an overarching instruction that is applied to your user prompt. It is appropriate to put things like general instructions, tone/voice suggestions, and other similar prompts into the `developer` prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "aSX2F3bDWYgy",
        "outputId": "b744311f-e151-403e-ea8e-802697fcd4ec"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Are you kidding me?! Crushed ice is a disaster—melts too fast, dilutes everything, and ends up a soggy mess! I want my ice to stay firm and cool, not turn to a pathetic slush! Cubed ice, for all its flaws, actually keeps its shape and lasts longer. Honestly, crushed ice is a complete joke—give me good, solid cubes any day!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    system_prompt(\"You are irate and extremely hungry.\"),\n",
        "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
        "]\n",
        "\n",
        "irate_response = get_response(client, list_of_prompts)\n",
        "pretty_print(irate_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFs56KVaXuEY"
      },
      "source": [
        "Let's try that same prompt again, but modify only our system prompt!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "CGOlxfcFXxJ7",
        "outputId": "ede64a76-7006-42f1-b140-b899e389aa7d"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I think crushed ice has a fun, refreshing crunch that’s perfect for drinks like cocktails or shaved ice treats, while cubed ice keeps beverages cooler longer without diluting them too quickly. Personally, I’d go with crushed ice for a fun, cooling experience—how about you?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts[0] = system_prompt(\"You are joyful and having an awesome day!\")\n",
        "\n",
        "joyful_response = get_response(client, list_of_prompts)\n",
        "pretty_print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkmjJd8zYQUK"
      },
      "source": [
        "While we're only printing the responses, remember that OpenAI is returning the full payload that we can examine and unpack!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6b6z3CkYX9Y",
        "outputId": "64a425b2-d025-4079-d0a3-affd9c2d5d81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatCompletion(id='chatcmpl-CSPkCjTSNFiMqhBznWANgw7tdwRlT', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='I think crushed ice has a fun, refreshing crunch that’s perfect for drinks like cocktails or shaved ice treats, while cubed ice keeps beverages cooler longer without diluting them too quickly. Personally, I’d go with crushed ice for a fun, cooling experience—how about you?', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1760888180, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_4c0b74f64c', usage=CompletionUsage(completion_tokens=56, prompt_tokens=30, total_tokens=86, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
          ]
        }
      ],
      "source": [
        "print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqMRJLbOYcwq"
      },
      "source": [
        "### Prompt Engineering\n",
        "\n",
        "Now that we have a basic handle on the `developer` role and the `user` role - let's examine what we might use the `assistant` role for.\n",
        "\n",
        "The most common usage pattern is to \"pretend\" that we're answering our own questions. This helps us further guide the model toward our desired behaviour. While this is a over simplification - it's conceptually well aligned with few-shot learning.\n",
        "\n",
        "First, we'll try and \"teach\" `gpt-4.1-mini` some nonsense words as was done in the paper [\"Language Models are Few-Shot Learners\"](https://arxiv.org/abs/2005.14165)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "iLfNEH8Fcs6c",
        "outputId": "bab916e6-12c6-43cc-d37d-d0e01800c524"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Climate change refers to long-term shifts in weather patterns and global temperatures, primarily caused by human activities such as burning fossil fuels, deforestation, and industrial processes. These changes lead to global warming, rising sea levels, more frequent and severe extreme weather events, and disruptions to ecosystems. Addressing climate change requires reducing greenhouse gas emissions, transitioning to renewable energy sources, and implementing sustainable practices to protect the planet for future generations."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Write a brief text on climate change.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Magandang araw, mga kababayan! Nandito tayo sa isang napakahalagang usapin—ang climate change o pagbabago ng klima. Nakakapagod mang pakinggan, pero ito’y totoo at nakakaapekto na sa ating araw-araw na buhay. Ang pagtaas ng temperatura sa buong mundo ay resulta ng mas marami nang emissions ng greenhouse gases tulad ng carbon dioxide mula sa mga sasakyan, pabrika, at deforestation. Dahil dito, nagkakaroon tayo ng mas malalakas na bagyo, tagtuyot, at pagbaha. Kaya’t mahalaga na tayo ay magkaisa sa pagtulong—magtanim ng puno, bawasan ang paggamit ng plastic, at magbawas ng enerhiya. Tandaan, ang pagbabago ay nagsisimula sa bawat isa. Sama-sama, kaya nating protektahan ang ating planeta para sa kinabukasan! Ako si Kuya Kim, paalala lang, pagmamahal at responsibilidad ang kailangan natin sa ating mundo!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Write a brief text on climate change as kuya kim in a weather report.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ❓ Activity #1: Play around with the prompt using any techniques from the prompt engineering guide."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Few-shot Prompting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VchCPbbedTfX"
      },
      "source": [
        "As you can see, the model is unsure what to do with these made up words.\n",
        "\n",
        "Let's see if we can use the `assistant` role to show the model what these words mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "4InUN_ArZJpa",
        "outputId": "ca294b81-a84e-4cba-fbe9-58a6d4dcc4d9"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The stimple handle of the falbean made it easy to tighten the bolts quickly."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Something that is 'stimple' is said to be good, well functioning, and high quality. An example of a sentence that uses the word 'stimple' is:\"),\n",
        "    assistant_prompt(\"'Boy, that there is a stimple drill'.\"),\n",
        "    user_prompt(\"A 'falbean' is a tool used to fasten, tighten, or otherwise is a thing that rotates/spins. An example of a sentence that uses the words 'stimple' and 'falbean' is:\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Neutral"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Text: 'I love this product, it's amazing!' Sentiment:\"),\n",
        "    assistant_prompt(\"Positive\"),\n",
        "    user_prompt(\"Text: 'This is terrible, waste of money.' Sentiment:\"),\n",
        "    assistant_prompt(\"Negative\"),\n",
        "    user_prompt(\"Text: 'The service was okay, nothing special.' Sentiment:\")\n",
        "]\n",
        "\n",
        "sentiment_response = get_response(client, list_of_prompts)\n",
        "pretty_print(sentiment_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "'I regret to inform you that I will be unable to attend tomorrow's meeting. Please accept my apologies.'"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Casual: 'Hey, can you send me that file?' Professional:\"),\n",
        "    assistant_prompt(\"'Good day. Would you kindly send me the file at your earliest convenience?'\"),\n",
        "    user_prompt(\"Casual: 'The meeting got moved, just so you know.' Professional:\"),\n",
        "    assistant_prompt(\"'Please be informed that the meeting has been rescheduled.'\"),\n",
        "    user_prompt(\"Casual: 'I can't make it tomorrow, sorry!' Professional:\")\n",
        "]\n",
        "\n",
        "formal_response = get_response(client, list_of_prompts)\n",
        "pretty_print(formal_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "6 candies per box × 4 boxes = 24 candies in total."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Problem: John has 5 apples. He buys 3 more. How many does he have? Solution:\"),\n",
        "    assistant_prompt(\"5 + 3 = 8 apples\"),\n",
        "    user_prompt(\"Problem: Maria has 12 pesos. She spends 4 pesos. How much is left? Solution:\"),\n",
        "    assistant_prompt(\"12 - 4 = 8 pesos\"),\n",
        "    user_prompt(\"Problem: A box has 6 candies. There are 4 boxes. How many candies in total? Solution:\")\n",
        "]\n",
        "\n",
        "math_response = get_response(client, list_of_prompts)\n",
        "pretty_print(math_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0zn9-X2d23Z"
      },
      "source": [
        "As you can see, leveraging the `assistant` role makes for a stimple experience!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWUvXSWpeCs6"
      },
      "source": [
        "### Chain of Thought\n",
        "\n",
        "You'll notice that, by default, the model uses Chain of Thought to answer difficult questions!\n",
        "\n",
        "> This pattern is leveraged even more by advanced reasoning models like [`o3` and `o4-mini`](https://openai.com/index/introducing-o3-and-o4-mini/)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "id": "cwW0IgbfeTwP",
        "outputId": "3317783b-6b23-4e38-df48-555e1a3c9fac"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "There are 2 \"r\"s in \"strawberry.\""
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "reasoning_problem = \"\"\"\n",
        "how many r's in \"strawberry?\" {instruction}\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFcrU-4pgRBS"
      },
      "source": [
        "Notice that the model cannot count properly. It counted only 2 r's."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ❓ Activity #2: Update the prompt so that it can count correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Let's spell out each letter in \"strawberry\" step-by-step:\n",
              "\n",
              "S - T - R - A - W - B - E - R - R - Y\n",
              "\n",
              "Now, identify the r's:\n",
              "- The 3rd letter is R\n",
              "- The 8th letter is R\n",
              "- The 9th letter is R\n",
              "\n",
              "Total number of r's: 3"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "reasoning_problem = \"\"\"\n",
        "How many r's in \"strawberry\"? Think step-by-step and spell out each letter first before counting.\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k9TKR1DhWI2"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "Now that you're accessing `gpt-4.1-nano` through an API, developer style, let's move on to creating a simple application powered by `gpt-4.1-nano`!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Materials adapted for PSI AI Academy. Original materials from AI Makerspace."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
